{"userInfo":{"institute":false,"member":false,"individual":false,"guest":false,"subscribedContent":false,"fileCabinetContent":false,"fileCabinetUser":false,"institutionalFileCabinetUser":false,"showPatentCitations":true,"showGet802Link":false,"showOpenUrlLink":false,"tracked":false},"references":[{"order":"1","text":"Y. Feng, Z. Chen, J.A. Jones, C. Fang, B. Xu, \"Test report prioritization to assist crowdsourced testing\", <em>Proceedings of the 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering (FSE 2015)</em>, pp. 225-236, 2015.","title":"Test report prioritization to assist crowdsourced testing","context":[{"sec":"sec1","text":" In crowdsourced testing, crowd workers are required to submit test reports after performing testing tasks in crowd-sourced platform [1].","part":"1"},{"sec":"sec2a","text":"The procedure of crowdsourced testing [1].","part":"1"},{"sec":"sec7a","text":" [1] proposed test report prioritization methods for use in crowdsourced testing.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2786805.2786862","abstract":" In crowdsourced testing, users can be incentivized to perform testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a financial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called &#34;test reports&#34; and are composed of simple natural language and screenshots. Back at the software-development organization, developers must manually inspect the test reports to judge their value for reve...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Test+report+prioritization+to+assist+crowdsourced+testing&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref1"},{"order":"2","text":"T. Menzies, A. Marcus, \"Automated severity assessment of software defect reports\", <em>Proceedings of IEEE International Conference onSoftware Maintenance (ICSM 2008)</em>, pp. 346-355, 2008.","title":"Automated severity assessment of software defect reports","context":[{"sec":"sec1","text":"Several existing researches have been proposed to classify issue reports of open source projects using supervised machine learning algorithms [2]–[5].","part":"1"},{"sec":"sec7b","text":" Menzies and Marcus [2] proposed an automated severity assessment method by text mining and machine learning techniques.","part":"1"}],"links":{"documentLink":"/document/4658083","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=4658083","abstract":"In mission critical systems, such as those developed by NASA, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue. The paper presents a new and automated method named SEVER...","pdfSize":"393KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Automated+severity+assessment+of+software+defect+reports&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref2"},{"order":"3","text":"Y. Tian, D. Lo, C. Sun, \"Drone: Predicting priority of reported bugs by multi-factor analysis\", <em>Proceedings of the 29th IEEE International Conference on Software Maintenance (ICSM 2013)</em>, pp. 200-209, 2013.","title":"Drone: Predicting priority of reported bugs by multi-factor analysis","context":[{"sec":"sec1","text":"Several existing researches have been proposed to classify issue reports of open source projects using supervised machine learning algorithms [2]–[3][5].","part":"1"},{"sec":"sec7b","text":" [3] proposed DRONE, a multi factor analysis technique to classify the priority of bug reports.","part":"1"}],"links":{"abstract":"Bugs are prevalent. To improve software quality, developers often allow users to report bugs that they found using a bug tracking system such as Bugzilla. Users would specify among other things, a description of the bug, the component that is affected by the bug, and the severity of the bug. Based on this information, bug triagers would then assign a priority level to the reported bug. As resources are limited, bug reports would be investigated based on their priority levels. This priority assig...","pdfSize":"280KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Drone%3A+Predicting+priority+of+reported+bugs+by+multi-factor+analysis&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref3"},{"order":"4","text":"X. Wang, L. Zhang, T. Xie, J. Anvik, J. Sun, \"An approach to detecting duplicate bug reports using natural language and execution information\", <em>Proceedings of the 30th International Conference on Software Engineering (ICSE 2008)</em>, pp. 461-470, 2008.","title":"An approach to detecting duplicate bug reports using natural language and execution information","context":[{"sec":"sec1","text":"Several existing researches have been proposed to classify issue reports of open source projects using supervised machine learning algorithms [2]–[4][5].","part":"1"},{"sec":"sec7b","text":" [4] proposed a technique combining natural language and execution information to detect duplicate failure reports.","part":"1"},{"sec":"sec8","text":" Hence, our approach can well motivate the various classification problems which have required plenty of labeled data, e.g., report classification [4], [5], app review classification [32], [33].","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/1368088.1368151","abstract":"An open source project typically maintains an open bug repository so that bug reports from all over the world can be gathered. When a new bug report is submitted to the repository, a person, called a triager, examines whether it is a duplicate of an existing bug report. If it is, the triager marks it as duplicate and the bug report is removed from consideration for further work. In the literature, there are approaches exploiting only natural language information to detect duplicate bug reports. ...","pdfSize":"286KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=An+approach+to+detecting+duplicate+bug+reports+using+natural+language+and+execution+information&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref4"},{"order":"5","text":"Y. Zhou, Y. Tong, R. Gu, H. Gall, \"Combining text mining and data mining for bug report classification\", <em>Proceedings of the 30th IEEE International Conference on Software Maintenance (ICSM 2014)2014</em>, pp. 311-320.","title":"Combining text mining and data mining for bug report classification","context":[{"sec":"sec1","text":"Several existing researches have been proposed to classify issue reports of open source projects using supervised machine learning algorithms [2]–[5].","part":"1"},{"sec":"sec7b","text":" [5] proposed a hybrid approach by combining both text mining and data mining techniques of bug report data to automate the classification process.","part":"1"},{"sec":"sec8","text":" Hence, our approach can well motivate the various classification problems which have required plenty of labeled data, e.g., report classification [4], [5], app review classification [32], [33].","part":"1"}],"links":{"abstract":"Misclassification of bug reports inevitably sacrifices the performance of bug prediction models. Manual examinations can help reduce the noise but bring a heavy burden for developers instead. In this paper, we propose a hybrid approach by combining both text mining and data mining techniques of bug report data to automate the prediction process. The first stage leverages text mining techniques to analyze the summary parts of bug reports and classifies them into three levels of probability. The e...","pdfSize":"458KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Combining+text+mining+and+data+mining+for+bug+report+classification&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref5"},{"order":"6","text":"B. Settles, \"Active learning literature survey\", <em>University of Wisconsin-Madison Tech. Rep.</em>, 2010.","title":"Active learning literature survey","context":[{"sec":"sec1","text":" We further analyze the deep reason and find that previous active learning techniques assumed data are identically distributed [6].","part":"1"},{"sec":"sec2b","text":" The key hypothesis is that, if the learning algorithm is allowedto choose the data from which it learns, it will perform better with less training data [6].","part":"1"},{"sec":"sec2b","text":" In this way, the active learner aims to achieve high accuracy using as few labeled instances as possible, thereby reducing the cost of obtaining labeled data [6].","part":"1"},{"sec":"sec3b","text":" The design of existing techniques is based on such assumption that data are identically distributed [6].","part":"1"},{"sec":"sec3b1","text":" Because of the existence of local bias, randomly choosing the initial instance in existing active learning techniques cannot work well [6].","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+learning+literature+survey&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref6"},{"order":"7","text":"D.D. Lewis, W.A. Gale, \"A sequential algorithm for training text classifiers\", <em>Proceedings of ACM-SIGIR Conference on Researchand Development in Information Retrieval (SIGIR 1994)</em>, 1994.","title":"A sequential algorithm for training text classifiers","context":[{"sec":"sec2b","text":" For example, the chosen instance is the one which the classifier is least certain how to label [7].","part":"1"}],"links":{"crossRefLink":"https://doi.org/10.1007/978-1-4471-2099-5_1","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=A+sequential+algorithm+for+training+text+classifiers&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref7"},{"order":"8","text":"A. Storkey, Dataset shift in machine learning, Cambridge:The MIT Press, 2009.","title":"Dataset shift in machine learning","context":[{"sec":"sec2c","text":"Local bias refers to the phenomenon that data are heterogeneous within dataset, and their distributions are often different among different parts of the dataset [8].","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=Dataset+shift+in+machine+learning&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref8"},{"order":"9","text":"I.H. Witten, E. Frank, Data Mining: Practical machine learning tools and techniques, Morgan Kaufmann, 2005.","title":"Data Mining: Practical machine learning tools and techniques","context":[{"sec":"sec2c","text":" We firstrandomly select an experimental project (P1, a map app) and use K-Means [9] togroup the reports into four clusters based on their technical terms (details are in Section 3.1).","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=Data+Mining%3A+Practical+machine+learning+tools+and+techniques&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref9"},{"order":"10","text":"Y. Yang, Z. He, K. Mao, Q. Li, V. Nguyen, B. Boehm, R. Valerdi, \"Analyzing and handling local bias for calibrating parametric cost estimation models\", <em>Information and Software Technology</em>, vol. 55, no. 8, pp. 1496-1511, 2013.","title":"Analyzing and handling local bias for calibrating parametric cost estimation models","context":[{"sec":"sec2c","text":"Local bias has been widely investigated in effort estimation and defect prediction studies [10]–[14].","part":"1"}],"links":{"crossRefLink":"https://doi.org/10.1016/j.infsof.2013.03.002","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Analyzing+and+handling+local+bias+for+calibrating+parametric+cost+estimation+models&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref10"},{"order":"11","text":"T. Zimmermann, N. Nagappan, H. Gall, E. Giger, B. Murphy, \"Cross-project defect prediction: A large scale experiment on data vs. domain vs. process\", <em>Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering (FSE 2009)</em>, pp. 91-100, 2009.","title":"Cross-project defect prediction: A large scale experiment on data vs. domain vs. process","context":[{"sec":"sec2c","text":"Local bias has been widely investigated in effort estimation and defect prediction studies [10]–[11][14].","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/1595696.1595713","abstract":"Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate th...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Cross-project+defect+prediction%3A+A+large+scale+experiment+on+data+vs.+domain+vs.+process&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref11"},{"order":"12","text":"T. Menzies, A. Butcher, A. Marcus, T. Zimmermann, D. Cok, \"Local vs. global models for effort estimation and defect prediction\", <em>Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)</em>, pp. 343-351, 2011.","title":"Local vs. global models for effort estimation and defect prediction","context":[{"sec":"sec2c","text":"Local bias has been widely investigated in effort estimation and defect prediction studies [10]–[12][14].","part":"1"}],"links":{"documentLink":"/document/6100072","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=6100072","abstract":"Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules. This instability can be explained by data heterogeneity. We show that effort and defect data contain many local regions with markedly different properties to the global space. In other words, what appears to be useful in a ...","pdfSize":"491KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Local+vs.+global+models+for+effort+estimation+and+defect+prediction&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref12"},{"order":"13","text":"T. Menzies, A. Butcher, A. Marcus, D. Cok, F. Shull, B. Turhan, T. Zimmermann, \"Local versus global lessons for defect prediction and effort estimation\", <em>IEEE Transactions on software engineering</em>, vol. 39, no. 6, pp. 822-834, 2013.","title":"Local versus global lessons for defect prediction and effort estimation","context":[{"sec":"sec2c","text":"Local bias has been widely investigated in effort estimation and defect prediction studies [10]–[13][14].","part":"1"},{"sec":"sec2c","text":" The most common technique to overcome the local bias isrelevancy filtering, e.g., nearest-neighbor similarity [13], [14].","part":"1"}],"links":{"documentLink":"/document/6363444","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=6363444","abstract":"Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local pro...","pdfSize":"2336KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Local+versus+global+lessons+for+defect+prediction+and+effort+estimation&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref13"},{"order":"14","text":"Z. He, F. Peters, T. Menzies, Y. Yang, \"Learning from open-source projects: An empirical study on defect prediction\", <em>Proceedings of ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM 2013)</em>, pp. 45-54, Oct 2013.","title":"Learning from open-source projects: An empirical study on defect prediction","context":[{"sec":"sec2c","text":"Local bias has been widely investigated in effort estimation and defect prediction studies [10]–[14].","part":"1"},{"sec":"sec2c","text":" The most common technique to overcome the local bias isrelevancy filtering, e.g., nearest-neighbor similarity [13], [14].","part":"1"}],"links":{"abstract":"The fundamental issue in cross project defect prediction is selecting the most appropriate training data for creating quality defect predictors. Another concern is whether historical data of open-source projects can be used to create quality predictors for proprietary projects from a practical point-of-view. Current studies have proposed statistical approaches to finding these training data, however, thus far no apparent effort has been made to study their success on proprietary data. Also these...","pdfSize":"501KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Learning+from+open-source+projects%3A+An+empirical+study+on+defect+prediction&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref14"},{"order":"15","text":"H. Finch, \"Comparison of distance measures in cluster analysis with dichotomous data\", <em>Journal of Data Science</em>, vol. 3, pp. 85-100, 2005.","title":"Comparison of distance measures in cluster analysis with dichotomous data","context":[{"sec":"sec3b1","text":" This is because prior study showed that it performs better for high-dimensional text documents than other distance measures (e.g., euclidean distance, manhattan distance) [15].","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=Comparison+of+distance+measures+in+cluster+analysis+with+dichotomous+data&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref15"},{"order":"16","text":"S.J. Huang, R. Jin, Z.H. Zhou, \"Active learning by querying informative and representative examples\", <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>.","title":"Active learning by querying informative and representative examples","context":[{"sec":"sec7c","text":"There is a wealth of active learning studies in machine learning literature, such as [16], [37].","part":"1"}],"links":{"documentLink":"/document/6747346","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=6747346","abstract":"Active learning reduces the labeling cost by iteratively selecting the most valuable data to query their labels. It has attracted a lot of interests given the abundance of unlabeled data and the high cost of labeling. Most active learning approaches select either informative or representative unlabeled instances to query their labels, which could significantly limit their performance. Although several active learning algorithms were proposed to combine the two query selection criteria, they are ...","pdfSize":"2113KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+learning+by+querying+informative+and+representative+examples&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref16"},{"order":"17","text":"B. Turhan, T. Menzies, A.B. Bener, J. Di Stefano, \"On the relative value of cross-company and within-company data for defect prediction\", <em>Empirical Software Engineering</em>, vol. 14, no. 5, Oct. 2009.","title":"On the relative value of cross-company and within-company data for defect prediction","context":[{"sec":"sec4c4","text":"Inspired by the cross- project prediction in defect prediction [17] and effort estimation [18], our experimental design is as follows: for each projectunder testing, we choose the most similar project from all available crowdsourced testing projects, then build a classifier based on the reports of that selected project.","part":"1"}],"links":{"crossRefLink":"https://doi.org/10.1007/s10664-008-9103-7","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=On+the+relative+value+of+cross-company+and+within-company+data+for+defect+prediction&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref17"},{"order":"18","text":"L.L. Minku, X. Yao, \"How to make best use of cross-company data in software effort estimation?\", <em>Proceedings of the 36th International Conference on Software Engineering (ICSE 2014)</em>, pp. 446-456, 2014.","title":"How to make best use of cross-company data in software effort estimation?","context":[{"sec":"sec4c4","text":"Inspired by the cross- project prediction in defect prediction [17] and effort estimation [18], our experimental design is as follows: for each projectunder testing, we choose the most similar project from all available crowdsourced testing projects, then build a classifier based on the reports of that selected project.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2568225.2568228","abstract":" Previous works using Cross-Company (CC) data for making Within-Company (WC) Software Effort Estimation (SEE) try to use CC data or models directly to provide predictions in the WC context. So, these data or models are only helpful when they match the WC context well. When they do not, a fair amount of WC training data, which are usually expensive to acquire, are still necessary to achieve good performance. We investigate how to make best use of CC data, so that we can reduce the amount of WC...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=How+to+make+best+use+of+cross-company+data+in+software+effort+estimation%3F&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref18"},{"order":"19","text":"S. Hido, T. Idé, H. Kashima, H. Kubo, H. Matsuzawa, \"Unsupervised change analysis using supervised learning\", <em>Proceedings of 12th Pacific-Asia Conference on Knowledge Discovery and Data Mining (PAKDD 2008)</em>, pp. 148-159, 2008.","title":"Unsupervised change analysis using supervised learning","context":[{"sec":"sec4c4","text":" The similarity between two projects is measured using the method in [19], which is based on the marginal distribution of training set and testset.","part":"1"}],"links":{"crossRefLink":"https://doi.org/10.1007/978-3-540-68125-0_15","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Unsupervised+change+analysis+using+supervised+learning&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref19"},{"order":"20","text":"S. Kotsiantis, \"Supervised machine learning: A review of classification techniques\", <em>Informatica</em>, vol. 31, pp. 249-268, 2007.","title":"Supervised machine learning: A review of classification techniques","context":[{"sec":"sec4c4","text":" We have experimented with Support Vector Machine (SVM) [20], Decision Tree [20], Naive Bayes [21], and Logistic Regression [21].","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=Supervised+machine+learning%3A+A+review+of+classification+techniques&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref20"},{"order":"21","text":"A. Berson, S. Smith, K. Thearling, \"An overview of data mining techniques\", <em>Building Data Mining Application for CRM 2004</em>.","title":"An overview of data mining techniques","context":[{"sec":"sec4c4","text":" We have experimented with Support Vector Machine (SVM) [20], Decision Tree [20], Naive Bayes [21], and Logistic Regression [21].","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=An+overview+of+data+mining+techniques&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref21"},{"order":"22","text":"C.D. Manning, P. Raghavan, H. Schlztze, Introduction to Information Retrieval, Cambridge University Press, 2008.","title":"Introduction to Information Retrieval","context":[{"sec":"sec4d","text":"The F-Measure of classifying true fault after termination, which is the harmonic mean of precision and recall [22], is used to measure the accuracy.","part":"1"}],"links":{"crossRefLink":"https://doi.org/10.1017/CBO9780511809071","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Introduction+to+Information+Retrieval&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref22"},{"order":"23","text":"K.J. Stol, B. Fitzgerald, \"Two's company three's a crowd: A case study of crowdsourcing software development\", <em>Proceedings of the 36th International Conference on Software Engineering (ICSE 2014)</em>, pp. 187-198, 2014.","title":"Two's company, three's a crowd: A case study of crowdsourcing software development","context":[{"sec":"sec7a","text":"Crowdsoucing is the activity of taking a job traditionally performed by a designated agent (usually an employee) and outsourcing it to an undefined, generally large group of people in the form of an open call [23].","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2568225.2568249","abstract":" Crowdsourcing is an emerging and promising approach which involves delegating a variety of tasks to an unknown workforce - the crowd. Crowdsourcing has been applied quite successfully in various contexts from basic tasks on Amazon Mechanical Turk to solving complex industry problems, e.g. InnoCentive. Companies are increasingly using crowdsourcing to accomplish specific software development tasks. However, very little research exists on this specific topic. This paper presents an in-depth in...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Two%27s+company%2C+three%27s+a+crowd%3A+A+case+study+of+crowdsourcing+software+development&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref23"},{"order":"24","text":"N. Chen, S. Kim, \"Puzzle-based automatic testing: Bringing humans into the loop by solving puzzles\", <em>Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering (ASE 2012)2012</em>, pp. 140-149.","title":"Puzzle-based automatic testing: Bringing humans into the loop by solving puzzles","context":[{"sec":"sec7a","text":" Chen and Kim [24] applied crowdsourced testing to testcase generation.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2351676.2351697","abstract":"Recently, many automatic test generation techniques have been proposed, such as Randoop, Pex and jCUTE. However, usually test coverage of these techniques has been around 50-60% only, due to several challenges, such as 1) the object mutation problem, where test generators cannot create and/or modify test inputs to desired object states; and 2) the constraint solving problem, where test generators fail to solve path conditions to cover certain branches. By analyzing branches not covered by state-...","pdfSize":"448KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Puzzle-based+automatic+testing%3A+Bringing+humans+into+the+loop+by+solving+puzzles&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref24"},{"order":"25","text":"R. Musson, J. Richards, D. Fisher, C. Bird, B. Bussone, S. Ganguly, \"Leveraging the crowd: How 48000 users helped improve lync performance\", <em>IEEE Software</em>, vol. 30, no. 4, pp. 38-45, 2013.","title":"Leveraging the crowd: How 48,000 users helped improve lync performance","context":[{"sec":"sec7a","text":" [25] proposed an approach, in which the crowd was used to measure real-world performance of software products.","part":"1"}],"links":{"documentLink":"/document/6509371","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=6509371","abstract":"Performance is a critical component of customer satisfaction with network-based applications. Unfortunately, accurately evaluating the performance of collaborative software that operates in extremely heterogeneous environments is difficult with traditional techniques such as modeling workloads or testing in controlled environments. To evaluate performance of an application in the wild during development, the authors deployed early versions of the software, collecting performance data from applic...","pdfSize":"1209KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Leveraging+the+crowd%3A+How+48%2C000+users+helped+improve+lync+performance&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref25"},{"order":"26","text":"V.H.M. Gomide, P.A. Valle, J.O. Ferreira, J.R.G. Barbosa, A.F. da Rocha, T.M.G.d.A. Barbosa, \"Affective crowdsourcing applied to usability testing\", <em>International Journal of Computer Scienceand Information Technologies</em>, vol. 5, no. 1, pp. 575-579, 2014.","title":"Affective crowdsourcing applied to usability testing","context":[{"sec":"sec7a","text":" [26] proposed an approach that employed a deterministic automata to help usability testing.","part":"1"}],"googleScholarLink":"https://scholar.google.com/scholar?as_q=Affective+crowdsourcing+applied+to+usability+testing&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref26"},{"order":"27","text":"M. Gómez, R. Rouvoy, B. Adams, L. Seinturier, \"Reproducing context-sensitive crashes of mobile apps using crowdsourced monitoring\", <em>Proceedings of the IEEE/ACM International Conference on Mobile Software Engineering and Systems (MOBIL ESoft)</em>, pp. 88-99, 2016.","title":"Reproducing context-sensitive crashes of mobile apps using crowdsourced monitoring","context":[{"sec":"sec7a","text":" [27] proposed MoTIF to detect and reproduce context-related crashes in mobile apps after their deployment in the wild.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2897073.2897088","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Reproducing+context-sensitive+crashes+of+mobile+apps+using+crowdsourced+monitoring&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref27"},{"order":"28","text":"J. Wang, Q. Cui, Q. Wang, S. Wang, \"Towards effectively test report classification to assist crowdsourced testing\", <em>Proceedings of ACM/IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM 2016)</em>, 2016.","title":"Towards effectively test report classification to assist crowdsourced testing","context":[{"sec":"sec7a","text":" Our previous work [28] proposed a cluster-based classification approach to effectively classify crowdsourced reports when facing with plenty of training data.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2961111.2962584","abstract":"Context: Automatic classification of crowdsourced test reports is important due to their tremendous sizes and large proportion of noises. Most existing approaches towards this problem focus on examining the performance of different machine learning or information retrieval techniques, and most are evaluated on open source dataset. However, our observation reveals that these approaches generate poor and unstable performances on real industrial crowdsourced testing data. We further analyze the ...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Towards+effectively+test+report+classification+to+assist+crowdsourced+testing&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref28"},{"order":"29","text":"M.S. Zanetti, I. Scholtes, C.J. Tessone, F. Schweitzer, \"Categorizing bugs with social networks: A case study on four open source software communities\", <em>Proceedings of the International Conference on Software Engineering (ICSE 2013)</em>, pp. 1032-1041, 2013.","title":"Categorizing bugs with social networks: A case study on four open source software communities","context":[{"sec":"sec7b","text":" [29] proposed a method to classify valid bug reports based on nine measures quantifying the social embeddedness of bug reporters in the collaboration network.","part":"1"}],"links":{"abstract":"Efficient bug triaging procedures are an important precondition for successful collaborative software engineering projects. Triaging bugs can become a laborious task particularly in open source software (OSS) projects with a large base of comparably inexperienced part-time contributors. In this paper, we propose an efficient and practical method to identify valid bug reports which a) refer to an actual software bug, b) are not duplicates and c) contain enough information to be processed right aw...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Categorizing+bugs+with+social+networks%3A+A+case+study+on+four+open+source+software+communities&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref29"},{"order":"30","text":"S. Wang, W. Zhang, Q. Wang, \"FixerCache: Unsupervised caching active developers for diverse bug triage\", <em>Proceedings of the InternationalSymposium on Empirical Software Engineering and Measurement (ESEM 2014)2014</em>, pp. 25:1-25:10.","title":"FixerCache: Unsupervised caching active developers for diverse bug triage","context":[{"sec":"sec7b","text":" [30] proposed FixerCache, an unsupervised approach for bug triage by caching developers based on their activeness in components of products.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2652524.2652536","abstract":"Context: Bug triage aims to recommend appropriate developers for new bugs in order to reduce time and effort in bug resolution. Most previous approaches for bug triage are supervised. Before recommending developers, these approaches need to learn developers&#39; bug-fix preferences via building and training models using text-information of developers&#39; historical bug reports. Goal: In this paper, we empirically address three limitations of supervised bug triage a...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=FixerCache%3A+Unsupervised+caching+active+developers+for+diverse+bug+triage&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref30"},{"order":"31","text":"K. Mao, Y. Yang, Q. Wang, Y. Jia, M. Harman, \"Developer recommendation for crowdsourced software development tasks\", <em>Proceedings of IEEE Symposium on Service-Oriented System Engineering (SOSE 2015)</em>, pp. 347-356, 2015.","title":"Developer recommendation for crowdsourced software development tasks","context":[{"sec":"sec7b","text":" [31] proposed content-based developer recommendation techniques for crowdsourcing tasks.","part":"1"}],"links":{"abstract":"Crowdsourced software development utilises an open call format to attract geographically distributed developers to accomplish various types of software development tasks. Although the open call format enables wide task accessibility, potential developers must choose from a dauntingly large set of task options (usually more than one hundred available tasks on TopCoder each day). Inappropriate developer-task matching may lower the quality of the software deliverables. In this paper, we employ cont...","pdfSize":"1217KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Developer+recommendation+for+crowdsourced+software+development+tasks&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref31"},{"order":"32","text":"W. Maalej, H. Nabil, \"Bug report feature request or simply praise? On automatically classifying app reviews\", <em>Proceedings of the 23rd IEEE InternationalRequirements Engineering Conference (RE 2015)</em>, pp. 116-125, 2015.","title":"Bug report, feature request, or simply praise? On automatically classifying app reviews","context":[{"sec":"sec7b","text":" [32]–[34], which can help deal with the large amounts of reviews.","part":"1"},{"sec":"sec8","text":" Hence, our approach can well motivate the various classification problems which have required plenty of labeled data, e.g., report classification [4], [5], app review classification [32], [33].","part":"1"}],"links":{"documentLink":"/document/7320414","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=7320414","abstract":"App stores like Google Play and Apple AppStore have over 3 Million apps covering nearly every kind of software and service. Billions of users regularly download, use, and review these apps. Recent studies have shown that reviews written by the users represent a rich source of information for the app vendors and the developers, as they include information about bugs, ideas for new features, or documentation of released features. This paper introduces several probabilistic techniques to classify a...","pdfSize":"461KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Bug+report%2C+feature+request%2C+or+simply+praise%3F+On+automatically+classifying+app+reviews&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref32"},{"order":"33","text":"E. Guzman, M. EI-Halaby, B. Bruegge, \"Ensemble methods for app review classification: An approach for software evolution\", <em>Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE 2015)</em>, 2015.","title":"Ensemble methods for app review classification: An approach for software evolution","context":[{"sec":"sec7b","text":"There were researches to classify app reviews as bug reports, feature requests, etc. [32]–[33][34], which can help deal with the large amounts of reviews.","part":"1"},{"sec":"sec8","text":" Hence, our approach can well motivate the various classification problems which have required plenty of labeled data, e.g., report classification [4], [5], app review classification [32], [33].","part":"1"}],"links":{"documentLink":"/document/7372065","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=7372065","abstract":"App marketplaces are distribution platforms for mobile applications that serve as a communication channel between users and developers. These platforms allow users to write reviews about downloaded apps. Recent studies found that such reviews include information that is useful for software evolution. However, the manual analysis of a large amount of user reviews is a tedious and time consuming task. In this work we propose a taxonomy for classifying app reviews into categories relevant for softw...","pdfSize":"165KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Ensemble+methods+for+app+review+classification%3A+An+approach+for+software+evolution&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref33"},{"order":"34","text":"S. Panichella, A.D. Sorbo, E. Guzman, C.A. Visaggio, G. Canfora, H.C. Gall, \"How can i improve my app? Classifying user reviews for software maintenance and evolution\", <em>Proceedings of the 31st IEEE International Conference on Software Maintenance (ICSM 2015)</em>, pp. 281-290, 2015.","title":"How can i improve my app? Classifying user reviews for software maintenance and evolution","context":[{"sec":"sec7b","text":"There were researches to classify app reviews as bug reports, feature requests, etc. [32]–[34], which can help deal with the large amounts of reviews.","part":"1"},{"sec":"sec7b","text":" Moreover, the performances of our approach surpass theirs (F-measure is 0.72 to 0.95 in [34]) a lot.","part":"1"}],"links":{"documentLink":"/document/7332474","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=7332474","abstract":"App Stores, such as Google Play or the Apple Store, allow users to provide feedback on apps by posting review comments and giving star ratings. These platforms constitute a useful electronic mean in which application developers and users can productively exchange information about apps. Previous research showed that users feedback contains usage scenarios, bug reports and feature requests, that can help app developers to accomplish software maintenance and evolution tasks. However, in the case o...","pdfSize":"680KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=How+can+i+improve+my+app%3F+Classifying+user+reviews+for+software+maintenance+and+evolution&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref34"},{"order":"35","text":"V. Avdiienko, K. Kuznetsov, A. Gorla, A. Zeller, S. Arzt, S. Rasthofer, E. Bodden, \"Mining apps for abnormal usage of sensitive data\", <em>Proceedings of the IEEE/ACM 37th IEEE International Conference on Software Engineering (ICSE 2015)</em>, vol. 1, pp. 426-436, May 2015.","title":"Mining apps for abnormal usage of sensitive data","context":[{"sec":"sec7b","text":"Some other studies focus on differentiating between malicious behaviors and benign behaviors of app, based on data flow or context information [35], [36].","part":"1"}],"links":{"abstract":"What is it that makes an app malicious? One important factor is that malicious apps treat sensitive data differently from benign apps. To capture such differences, we mined 2,866 benign Android applications for their data flow from sensitive sources, and compare these flows against those found in malicious apps. We find that (a) for every sensitive source, the data ends up in a small number of typical sinks; (b) these sinks differ considerably between benign and malicious apps; (c) these differe...","pdfSize":"987KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Mining+apps+for+abnormal+usage+of+sensitive+data&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref35"},{"order":"36","text":"W. Yang, X. Xiao, B. Andow, S. Li, T. Xie, W. Enck, \"AppContext: Differentiating malicious and benign mobile app behaviors using context\", <em>Proceedings of the 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering (ICSE 2015)</em>, vol. 1, pp. 303-313, May 2015.","title":"AppContext: Differentiating malicious and benign mobile app behaviors using context","context":[{"sec":"sec7b","text":"Some other studies focus on differentiating between malicious behaviors and benign behaviors of app, based on data flow or context information [35], [36].","part":"1"}],"links":{"abstract":"Mobile malware attempts to evade detection during app analysis by mimicking security-sensitive behaviors of benign apps that provide similar functionality (e.g., sending SMS messages), and suppressing their payload to reduce the chance of being observed (e.g., executing only its payload at night). Since current approaches focus their analyses on the types of security-sensitive resources being accessed (e.g., network), these evasive techniques in malware make differentiating between malicious and...","pdfSize":"1198KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=AppContext%3A+Differentiating+malicious+and+benign+mobile+app+behaviors+using+context&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref36"},{"order":"37","text":"S. Chakraborty, V. Balasubramanian, A.R. Sankar, S. Panchanathan, J. Ye, \"Batchrank: A novel batch mode active learning framework for hierarchical classification\", <em>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2015)</em>, pp. 99-108, 2015.","title":"Batchrank: A novel batch mode active learning framework for hierarchical classification","context":[{"sec":"sec7c","text":"There is a wealth of active learning studies in machine learning literature, such as [16], [37].","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2783258.2783298","abstract":"Active learning algorithms automatically identify the salient and exemplar instances from large amounts of unlabeled data and thus reduce human annotation effort in inducing a classification model. More recently, Batch Mode Active Learning (BMAL) techniques have been proposed, where a batch of data samples is selected simultaneously from an unlabeled set. Most active learning algorithms assume a flat label space, that is, they consider the class labels to be independent. However, in many appl...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Batchrank%3A+A+novel+batch+mode+active+learning+framework+for+hierarchical+classification&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref37"},{"order":"38","text":"J.F. Bowring, J.M. Rehg, M.J. Harrold, \"Active learning for automatic classification of software behavior\", <em>Proceedings of the ACM SIGSOFT InternationalSymposium on Software Testing and Analysis (ISSTA 2004)</em>, pp. 195-205, 2004.","title":"Active learning for automatic classification of software behavior","context":[{"sec":"sec7c","text":" [38] proposed an automatic approach for classifying program behavior by leveraging Markov model and active learning.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/1007512.1007539","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+learning+for+automatic+classification+of+software+behavior&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref38"},{"order":"39","text":"Lucia D. Lo, L. Jiang, A. Budi, \"Active refinement of clone anomaly reports\", <em>Proceedings of the 34th International Conference on Software Engineering (ICSE 2012)</em>, pp. 397-407, June 2012.","title":"Active refinement of clone anomaly reports","context":[{"sec":"sec7c","text":" [39] proposed an approach to actively incorporate user feedback in ranking clone anomaly reports.","part":"1"}],"links":{"abstract":" Software clones have been widely studied in the recent literature and shown useful for finding bugs because inconsistent changes among clones in a clone group may indicate potential bugs. However, many inconsistent clone groups are not real bugs. The excessive number of false positives could easily impede broad adoption of clone-based bug detection approaches.   In this work, we aim to improve the usability of clonebased bug detection tools by increasing the rate of true positives fou...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+refinement+of+clone+anomaly+reports&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref39"},{"order":"40","text":"S. Wang, D. Lo, L. Jiang, \"Active Code search: Incorporating user feedback to improve code search relevance\", <em>Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering (ASE 2014)</em>, pp. 677-682, 2014.","title":"Active Code search: Incorporating user feedback to improve code search relevance","context":[{"sec":"sec7c","text":" [40] proposed a technique that can refine the results from a code search engine by actively incorporating incremental user feedback.","part":"1"}],"links":{"acmLink":"https://doi.org/10.1145/2642937.2642947","abstract":"Code search techniques return relevant code fragments given a user query. They typically work in a passive mode: given a user query, a static list of code fragments sorted by the relevance scores decided by a code search technique is returned to the user. A user will go through the sorted list of returned code fragments from top to bottom. As the user checks each code fragment one by one, he or she will naturally form an opinion about the true relevance of the code fragment. In an acti...","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+Code+search%3A+Incorporating+user+feedback+to+improve+code+search+relevance&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref40"},{"order":"41","text":"F. Thung, X.-B.D. Le, D. Lo, \"Active semi-supervised defect categorization\", <em>Proceedings of the 23rd International Conference on Program Comprehension (ICPC 2015)</em>, pp. 60-70, 2015.","title":"Active semi-supervised defect categorization","context":[{"sec":"sec7c","text":" [41] proposed an active semi-supervised defect prediction approach to classify defects into ODC defect type.","part":"1"}],"links":{"documentLink":"/document/7181433","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=7181433","abstract":"Defects are inseparable part of software development and evolution. To better comprehend problems affecting a software system, developers often store historical defects and these defects can be categorized into families. IBM proposes Orthogonal Defect Categorization (ODC) which include various classifications of defects based on a number of orthogonal dimensions (e.g., Symptoms and semantics of defects, root causes of defects, etc.). To help developers categorize defects, several approaches that...","pdfSize":"231KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+semi-supervised+defect+categorization&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref41"},{"order":"42","text":"S. Ma, S. Wang, D. Lo, R.H. Deng, C. Sun, \"Active semi-supervised approach for checking app behavior against its description\", <em>Proceedings of the 39th Annual ComputerSoftware and Applications Conference (COMPSAC 2015)</em>, vol. 2, pp. 179-184, July 2015.","title":"Active semi-supervised approach for checking app behavior against its description","context":[{"sec":"sec7c","text":" [42] proposed an active approach for detecting malicious apps.","part":"1"}],"links":{"abstract":"Mobile applications are popular in recent years. They are often allowed to access and modify users&#39; sensitive data. However, many mobile applications are malwares that inappropriately use these sensitive data. To detect these malwares, Gorla et al. Propose CHABADA which compares app behaviors against its descriptions. Data about known malwares are not used in their work, which limits its effectiveness. In this work, we extend the work by Gorla et al. By proposing an active and semi-supervised ap...","pdfSize":"412KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+semi-supervised+approach+for+checking+app+behavior+against+its+description&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref42"},{"order":"43","text":"E. Kocaguneli, T. Menzies, J. Keung, D. Cok, R. Madachy, \"Active learning and effort estimation: Finding the essential content of software effort estimation data\", <em>IEEE Transactions on Software Engineering</em>, vol. 39, no. 8, pp. 1040-1053, Aug 2013.","title":"Active learning and effort estimation: Finding the essential content of software effort estimation data","context":[{"sec":"sec7c","text":" [43] proposed QUICK, which is an active learning method that assists in finding the essential content of software effort estimation data, so as to simplify the complex estimation methods.","part":"1"}],"links":{"documentLink":"/document/6392173","pdfLink":"/stamp/stamp.jsp?tp=&arnumber=6392173","abstract":"Background: Do we always need complex methods for software effort estimation (SEE)? Aim: To characterize the essential content of SEE data, i.e., the least number of features and instances required to capture the information within SEE data. If the essential content is very small, then 1) the contained information must be very brief and 2) the value added of complex learning schemes must be minimal. Method: Our QUICK method computes the euclidean distance between rows (instances) and columns (fe...","pdfSize":"2308KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=Active+learning+and+effort+estimation%3A+Finding+the+essential+content+of+software+effort+estimation+data&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref43"},{"order":"44","text":"J. Nam, S. Kim, \"CLAMI: Defect prediction on unlabeled datasets\", <em>Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering (ASE 2015)</em>, pp. 452-463, Nov 2015.","title":"CLAMI: Defect prediction on unlabeled datasets","context":[{"sec":"sec7c","text":" [44] proposed novel approaches to conduct defect prediction on unlabeled datasets in an automated manner.","part":"1"}],"links":{"abstract":"Defect prediction on new projects or projects with limited historical data is an interesting problem in software engineering. This is largely because it is difficult to collect defect information to label a dataset for training a prediction model. Cross-project defect prediction (CPDP) has tried to address this problem by reusing prediction models built by other projects that have enough historical data. However, CPDP does not always build a strong prediction model because of the different distr...","pdfSize":"1127KB","openUrlImgLoc":"/assets/img/btn.find-in-library.png"},"googleScholarLink":"https://scholar.google.com/scholar?as_q=CLAMI%3A+Defect+prediction+on+unlabeled+datasets&as_occt=title&hl=en&as_sdt=0%2C31","refType":"biblio","id":"ref44"}],"formulaStrippedArticleTitle":"Local-based active classification of test report to assist crowdsourced testing","getProgramTermsAccepted":false,"title":"Local-based active classification of test report to assist crowdsourced testing","standardTitle":"Local-based active classification of test report to assist crowdsourced testing","pubLink":"/xpl/conhome/7579013/proceeding","issueLink":"/xpl/tocresult.jsp?isnumber=null","allowComments":false,"isJournal":false,"isConference":true,"isBook":false,"isGetArticle":false,"isGetAddressInfoCaptured":false,"isMarketingOptIn":false,"xploreDocumentType":"Conference Publication","applyOUPFilter":false,"isStandard":false,"publisher":"IEEE","isOpenAccess":false,"isChapter":false,"isStaticHtml":true,"htmlLink":"/document/null/","isEarlyAccess":false,"isProduct":false,"isEphemera":false,"isMorganClaypool":false,"persistentLink":"https://ieeexplore.ieee.org/servlet/opac?punumber=7579013","isACM":false,"isSMPTE":false,"isFreeDocument":false,"isSAE":false,"isNow":false,"isCustomDenial":false,"isPromo":false,"isNotDynamicOrStatic":false,"htmlAbstractLink":"/document/null/","isOUP":false,"isDynamicHtml":true,"openAccessFlag":"F","ephemeraFlag":"false","title":"Local-based active classification of test report to assist crowdsourced testing","html_flag":"true","ml_html_flag":"true","mlTime":"PT0.645184S","publisher":"IEEE","lastupdate":"2019-06-17","mediaPath":"/mediastore_new/IEEE/content/media/7579013/7582735/7582757","contentType":"conferences","definitions":"false","publicationNumber":"7579013"}